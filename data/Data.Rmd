```{r load-packages, message = FALSE}
library(tidyverse)
library(ggplot2)
library(broom)
library(readxl)
library(tidytext)
library(dplyr)
library(tm)
library(SnowballC)
library(lubridate)
```
```{r read_in_data}
Palestine_news_articles <- read_xlsx("../data/Palestine_news_articles.xlsx")
Israel_news_articles <- read_xlsx("../data/israel_news_articles.xlsx")
Gaza_news_articles <- read_xlsx("../data/gaza_news_articles.xlsx")
chat_gpt_article_headlines <- read_xlsx("../data/chat_gpt_data (1).xlsx")
additional_news_data <- read_csv("../data/news_data.csv")
million_news <- read_csv("../data/abcnews-date-text 2 (1).csv")
```
```{r initial_tidy}
tidy_pna <- Palestine_news_articles %>% 
  select("...2", "...3", "...4", "...5") %>% 
  rename("title" = "...2",
         "journal" = "...3",
         "date" = "...4",
         "description" = "...5") %>% 
  filter(!is.na(title)) %>% 
  filter(title != "Title",
         journal != "Journal",
         date != "Date",
         description != "Description") %>% 
  filter(!str_starts(journal, "#")) %>% 
  mutate(keyword = "palestine") %>% 
  distinct()
```
```{r initial_tidy2}
tidy_ina <- Israel_news_articles %>% 
  select("...2", "...3", "...4", "...5") %>% 
  rename("title" = "...2",
         "journal" = "...3",
         "date" = "...4",
         "description" = "...5") %>% 
  filter(!is.na(title)) %>% 
  filter(title != "Title",
         journal != "Journal",
         date != "Date",
         description != "Description") %>% 
  filter(!str_starts(journal, "#")) %>% 
  mutate(keyword = "israel") %>% 
  distinct()
```
```{r initial_tidy3}
tidy_gna <- Gaza_news_articles %>% 
  select("...2", "...3", "...4", "...5") %>% 
  rename("title" = "...2",
         "journal" = "...3",
         "date" = "...4",
         "description" = "...5") %>% 
  filter(!is.na(title)) %>% 
  filter(title != "Title",
         journal != "Journal",
         date != "Date",
         description != "Description") %>% 
  filter(!str_starts(journal, "#")) %>% 
  mutate(keyword = "gaza") %>% 
  distinct()
```

```{r pidf and all_data join}
pidf <- full_join(tidy_pna, tidy_ina)
all_data <- full_join(pidf, tidy_gna)
```

```{r most common journals}
tidy_pna %>%
  count(journal) %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(journal, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Journal", y = "Frequency") +
  theme_minimal()

tidy_ina %>%
  count(journal) %>%
  top_n(10, n) %>%
  ggplot(aes(x = reorder(journal, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Journal", y = "Frequency") +
  theme_minimal()
```

```{r unimportatnt test}
pidf %>%
  group_by(title, journal, description) %>%
  summarize(count_of_string = n()) #%>% 
  # mutate(count_of_string = n) %>%
  #mutate(keyword = if_else(count_of_string == 1, keyword, "Both"))
```

```{r more joining}
inner_join_df <- tidy_pna %>% 
  inner_join(tidy_ina, by = c("title", "journal", "description")) %>% 
  mutate(keyword = "Both") %>% 
  select(title, journal, description, keyword)
```

```{r ^ and create tokenized df}
tokenized_df <- left_join(pidf, inner_join_df, by = c("title", "journal", "description"))

  colnames(tokenized_df)[5] ="keyword"
  colnames(tokenized_df)[6] ="in_both"
  
tokenized_df <- tokenized_df %>% 
  distinct() %>% 
  mutate(in_both = if_else(in_both == "Both", T, F))
```

```{r All_tokens}
sentiments <- get_sentiments("bing")
stop_words <- get_stopwords()

tokenized_df <- tokenized_df %>%
  mutate(Title = title) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)

tokenized_df2 <- tokenized_df %>%
  mutate(Description = description) %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words)

tokenized_df <- tokenized_df %>%
  mutate(title_or_description = "title")

tokenized_df2 <- tokenized_df2 %>%
  mutate(title_or_description = "description")

colnames(tokenized_df)[3] ="Description"

All_tokens <- tokenized_df %>%
  full_join(tokenized_df2, by = c("journal", "keyword", "in_both", "Title", "word", "Description", "title_or_description"))
  
```

```{r initial_word_frequency_plot}
tokenized_df %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 20) %>%
    ggplot(aes(n, fct_reorder(word, n))) +
    geom_col()

# data$word <- wordStem(data$word)
```

```{r sentiment analysis that doesnt work}
tokenized_df %>% 
  inner_join(sentiments) %>% 
  group_by(sentiment) %>% 
  filter(in_both == F) %>% 
  filter(keyword == "palestine") %>% 
  count(sentiment, word, sort = T) %>% 
  filter(sentiment == "positive")

tokenized_df %>% 
  inner_join(sentiments) %>% 
  group_by(sentiment) %>% 
  filter(in_both == F) %>% 
  filter(keyword == "israel") %>% 
  count(sentiment, word, sort = T) %>% 
  filter(sentiment == "positive")
```

```{r}
pidf %>% 
  unnest_tokens(word, title) %>%
  anti_join(stop_words) %>% 
  group_by(journal) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  #arrange(desc(word_percentage)) %>% 
  mutate(weighted_percentage = word_percentage * log(total_words)) %>%  # Applying weighting to 
  #arrange(desc(weighted_percentage)) %>% 
  arrange(desc(word_percentage)) %>% 
  #filter(!(word %in% c("palestine", "israel", "gaza")))
  filter(word == "hamas") %>% 
  filter(n >= 100)
  #unnest_tokens(word, description)
```
```{r}
# All_tokens %>% 
#   group_by(word) %>%  
#   summarize(word_count = n(), journal, date, Description, keyword, in_both, Title, word, title_or_description) %>% 
#   mutate(total_words = sum(n)) %>% 
#   mutate(word_percentage = n/total_words) %>% 
#   mutate(weighted_percentage = word_percentage * log(total_words)) %>%
#   arrange(desc(word_percentage)) %>% 
#   group_by(word)
```

```{r}
tokenized_df %>% 
  group_by(keyword) %>% 
  count(word, sort = TRUE) %>% 
  filter(word %in% c("palestine", "israel")) %>% 
  ggplot(aes(x = keyword, y = n, fill = word)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Words Frequency in Title")

tokenized_df2 %>% 
  group_by(keyword) %>% 
  count(word, sort = TRUE) %>% 
  filter(word %in% c("palestine", "israel")) %>% 
  ggplot(aes(x = keyword, y = n, fill = word)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Words Frequency in Description")
```



```{r}
All_tokens %>% 
  group_by(keyword, title_or_description) %>% 
  count(word, sort = TRUE) %>% 
  filter(word %in% c("palestine", "israel")) %>% 
  ggplot(aes(x = keyword, y = n, fill = word)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = 'Frequency of Words "Israel" and "Palestine"') +
  xlab("News Article Subject") +
  ylab("Frequency") +
  facet_wrap(~title_or_description, nrow = 2)
```
```{r}
All_tokens %>% 
  group_by(keyword, title_or_description) %>% 
  count(word, sort = TRUE) %>% 
  filter(word %in% c("gaza", "west", "bank")) %>% 
  ggplot(aes(x = keyword, y = n, fill = word)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = 'Frequency of Words "Israel" and "Palestine"') +
  xlab("News Article Subject") +
  ylab("Frequency") +
  facet_wrap(~title_or_description, nrow = 2)
```

```{r}
gpt <- chat_gpt_article_headlines

pgpt <- chat_gpt_article_headlines %>% 
  select(Palestine_headings)

igpt <- chat_gpt_article_headlines %>% 
  select(Israel_headings)
```

```{r}
pgpt %>% 
  unnest_tokens(word, Palestine_headings) %>% 
  anti_join(stop_words) %>% 
  inner_join(sentiments) %>% 
  group_by(sentiment) %>% 
  count(sentiment, word, sort = T)

igpt %>% 
  unnest_tokens(word, Israel_headings) %>% 
  anti_join(stop_words) %>% 
  inner_join(sentiments) %>% 
  group_by(sentiment) %>% 
  count(sentiment, word, sort = T)
```
```{r}
distinct_igpt <- igpt %>% 
  distinct()

distinct_pgpt <- pgpt %>% 
  distinct()
```

```{r}
palestine_million <- million_news %>% 
  filter(str_detect(headline_text, fixed("palestine", ignore_case = T)) |
           str_detect(headline_text, fixed("palestinian", ignore_case = T)) |
           str_detect(headline_text, fixed("gaza", ignore_case = T)) |
           str_detect(headline_text, fixed("west bank", ignore_case = T))) %>% 
  mutate(keyword = "palestine")

israel_million <- million_news %>% 
  filter(str_detect(headline_text, fixed("israel", ignore_case = T))) %>% 
  mutate(keyword = "israel")
           #str_detect(headline_text, fixed("tel aviv", ignore_case = T)))

million_df <- full_join(palestine_million, israel_million, by = c("publish_date", "headline_text", "keyword"))
```

```{r}
additional_news_data <- additional_news_data %>%
  rename(title = headline) %>% 
  mutate(source = "additional_news_data")

million_df <- million_df %>%
  rename(title = headline_text,
         date = publish_date) %>% 
  mutate(source = "million_news")

million_df$date <- ymd(million_df$date)

million_df$date <- format(million_df$date, "%d-%m-%Y")

joining <- full_join(additional_news_data, million_df, by = c("title", "date", "source"))
```

```{r}
pidf <- pidf %>% 
  mutate(source = "my_scaped_data")

all_real_data <- pidf %>% 
  #select(!date) %>% 
  full_join(joining, by = c("title", "description", "keyword", "source")) %>% 
  rename(date = date.y, untidy_date = date.x) 


tokenized_df3 <- all_real_data %>%
  mutate(Title = title) %>%
  unnest_tokens(word, title) %>%
  anti_join(stop_words)

tokenized_df4 <- all_real_data %>%
  mutate(Description = description) %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words)

tokenized_df3 <- tokenized_df3 %>%
  mutate(title_or_description = "title")

tokenized_df4 <- tokenized_df4 %>%
  mutate(title_or_description = "description")

colnames(tokenized_df3)[3] ="Description"
colnames(tokenized_df4)[1] ="Title"

all_real_data_tokens <- tokenized_df3 %>%
  full_join(tokenized_df4)
```

```{r}
all_real_data_tokens %>%
  filter(!is.na(word)) %>% 
  filter(!is.na(keyword)) %>% 
  group_by(keyword) %>%
  filter(keyword == "israel") 

all_real_data_tokens %>%
  filter(!is.na(word)) %>% 
  filter(!is.na(keyword)) %>% 
  group_by(keyword) %>%
  filter(keyword == "palestine")

all_real_data_tokens %>%
  filter(!is.na(word)) %>% 
  filter(!is.na(keyword)) %>% 
  group_by(keyword) %>%
    count(word, sort = TRUE) %>%
    slice_max(n, n = 20) %>%
    ggplot(aes(n, fct_reorder(word, n), fill = keyword)) +
    geom_col()
```

```{r}
all_real_data <- all_real_data %>% 
  mutate(year = str_sub(date, 7, 10)) #%>% 
  #mutate(year = as.numeric(year))

all_real_data_tokens <- all_real_data_tokens %>%
  mutate(year = str_sub(date, 7, 10)) %>%
  mutate(year = as.numeric(year))

all_real_data_tokens %>%
  #filter(!is.na(year)) %>%
  count(year) %>%
  ggplot(aes(x = year, y = n)) +
  geom_col() +
  scale_x_continuous(breaks = seq(2003, 2023, by = 1))
```
```{r}
all_real_data_tokens %>% 
  group_by(year) %>% 
  #filter(source == "million_news")
  filter(!is.na(year), !is.na(word)) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  filter(word == "palestine") %>% 
  group_by(word) %>% 
  ggplot(aes(x = year, y = word_percentage)) +
  geom_line() +
  scale_x_continuous(breaks = seq(2003, 2024, by = 2)) +
  geom_vline(xintercept = 2009, colour = "red") +
  geom_vline(xintercept = 2017, colour = "red") +
  labs(title = )

all_real_data_tokens %>% 
  filter(!is.na(keyword)) %>% 
  group_by(year, keyword) %>% 
  filter(!is.na(year), !is.na(word)) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  filter(word %in% c("palestine")) %>% 
  group_by(word) %>% 
  ggplot(aes(x = year, y = word_percentage, colour = keyword)) +
  geom_line()+
  #geom_smooth()+
  scale_x_continuous(breaks = seq(2003, 2024, by = 1))
```
```{r}
all_real_data_tokens %>% 
  group_by(year) %>% 
  #filter(source == "million_news") %>% 
  filter(!is.na(year), !is.na(word)) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  filter(word == "palestine") %>% 
  group_by(word) %>% 
  ggplot(aes(x = year, y = word_percentage)) +
  geom_line() +
  scale_x_continuous(breaks = seq(2003, 2024, by = 1)) +
  geom_vline(xintercept = 2009, colour = "red") +
  geom_vline(xintercept = 2017, colour = "red") +
  labs(title = 'Percentage of the Word "Palestine" in Palestine/Israel News Article Headings Over Time') +
  xlab("Year")+
  ylab("Word Percentage")
```


It appears that between between 2009 to 2011 there is a large increase in the use of the word palestine particularly in news articles about palestine. Why? 

2009-2010: Settlement Freeze
U.S. President Barack Obama attempted to revive Israeli-Palestinian peace talks shortly after taking office in 2009. At a speech at Cairo University that year, Obama reiterated his support for a two-state solution.
Why It Matters: As part of a good faith gesture, Israeli Prime Minister Benjamin Netanyahu implemented a settlement freeze, a key Palestinian demand, that lasted 10 months. While talks briefly restarted, Palestinian Authority President Mahmoud Abbas aborted the talks.

No American president ever came into office with a better understanding of the tragic history of the Palestinians or a deeper commitment to help them achieve independence than Obama. In his Cairo speech in April 2009, Obama solemnly pledged to do everything in his power to bring about Palestinian statehood. - Al Jazeera 

Trump was elected in 2017


The 2013–2014 Israeli–Palestinian peace talks were part of the Israeli–Palestinian peace process. Direct negotiations between Israel and the Palestinians began on 29 July 2013 following an attempt by United States Secretary of State John Kerry to restart the peace process.

```{r}
all_real_data_tokens %>% 
  group_by(year) %>% 
  filter(!is.na(year), !is.na(word)) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  filter(word %in% c("terrorism")) %>% 
  group_by(word) %>% 
  ggplot(aes(x = year, y = word_percentage)) +
  geom_line()+
  scale_x_continuous(breaks = seq(2003, 2023, by = 1)) +
  geom_vline(xintercept = 2009, colour = "red") +
  geom_vline(xintercept = 2017, colour = "red") +
  geom_smooth()

all_real_data_tokens %>% 
  group_by(year, keyword) %>% 
  filter(!is.na(year), !is.na(word)) %>% 
  count(word, sort = TRUE) %>% 
  mutate(total_words = sum(n)) %>% 
  mutate(word_percentage = n/total_words) %>% 
  filter(word %in% c("terrorism")) %>% 
  group_by(word) %>% 
  ggplot(aes(x = year, y = word_percentage, colour = keyword)) +
  geom_point()+
  #geom_smooth()+
  scale_x_continuous(breaks = seq(2003, 2023, by = 1))
```


```{r}
All_tokens %>% 
  filter(title_or_description == "title")
All_tokens %>% 
  filter(title_or_description == "description")

All_tokens%>%
  filter(!is.na(word)) %>% 
  filter(!is.na(keyword)) %>% 
  group_by(keyword) %>%
  count(word, sort = TRUE) %>%
  ggplot(aes(x = n, y = n, colour = keyword)) +
  geom_point()
```

```{r fix date in scrape data}
 all_real_data_tokens <-  all_real_data_tokens %>% 
  mutate(fixed_date = case_when(
    str_detect(untidy_date, fixed("2023")) ~ 2023,
    str_detect(untidy_date, fixed("2022")) ~ 2022,
    str_detect(untidy_date, fixed("2021")) ~ 2021,
    str_detect(untidy_date, fixed("2020")) ~ 2020,
    str_detect(untidy_date, fixed("2019")) ~ 2019,
    str_detect(untidy_date, fixed("2018")) ~ 2018,
    str_detect(untidy_date, fixed("2017")) ~ 2017,
    str_detect(untidy_date, fixed("2016")) ~ 2016,
    str_detect(untidy_date, fixed("2015")) ~ 2015,
    str_detect(untidy_date, fixed("2014")) ~ 2014,
    str_detect(untidy_date, fixed("2013")) ~ 2013,
    str_detect(untidy_date, fixed("2012")) ~ 2012,
    str_detect(untidy_date, fixed("2011")) ~ 2011,
    str_detect(untidy_date, fixed("2010")) ~ 2010,
    str_detect(untidy_date, fixed("2009")) ~ 2009,
    str_detect(untidy_date, fixed("2008")) ~ 2008,
    str_detect(untidy_date, fixed("2007")) ~ 2007,
    str_detect(untidy_date, fixed("2006")) ~ 2006,
    str_detect(untidy_date, fixed("2005")) ~ 2005,
    str_detect(untidy_date, fixed("2004")) ~ 2004,
    str_detect(untidy_date, fixed("2003")) ~ 2003,
    str_detect(untidy_date, fixed("2002")) ~ 2002,
    str_detect(untidy_date, fixed("2001")) ~ 2001,
    str_detect(untidy_date, fixed("2000")) ~ 2000,
    is.na(untidy_date) ~ NA,
    .default = 2024
  )) %>% 
  mutate(year = if_else(is.na(year), fixed_date, year))
```

```{r}
additional_news_data <- additional_news_data %>% 
  mutate(year = str_sub(date, 7, 10))
```

```{r}
all_real_data_tokens %>% 
  filter(is.na(keyword))
```

```{r}
all_real_data_tokens %>% 
  filter(year == 2023) %>% 
  mutate(word = str_squish(word)) %>% 
  filter(word == "israel")
  
  # all_real_data_tokens$word <- gsub("[[:space:]]", "", all_real_data_tokens$word)
  # 
  all_real_data_tokens$word <- str_replace_all(all_real_data_tokens$word, "­", "")
```



